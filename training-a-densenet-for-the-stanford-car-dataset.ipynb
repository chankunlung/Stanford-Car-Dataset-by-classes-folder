{"cells":[{"metadata":{"_uuid":"8f0bf6ad3215f4d464e62d191b4d14301b77023c"},"cell_type":"markdown","source":"## Training a DenseNet for the Stanford Car dataset\nI will train a DenseNet 121 for the dataset I uploaded. This is an example of using Keras and ImageDataGenerator for classification algorithms.\n\n### Loading libraries\nFirs, we load the basic libraries (numpy, os, pandas, matplotlib...)"},{"metadata":{"trusted":true,"_uuid":"176de3caaacdc99d28e9e6492df263bdf65212c8","collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport pandas as pd\nimport pickle\nimport csv\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport tensorflow as tf\nfrom PIL import Image\n\nimport os\nprint(os.listdir(\"../input/stanford-car-dataset-by-classes-folder\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6d94da00f86159e69b2003e9090a895ea7cfd65"},"cell_type":"markdown","source":"Now we set the base path for the car dataset and show one image of the dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"path_base = '../input/stanford-car-dataset-by-classes-folder'\n\nimage = Image.open(path_base + '/car_data/train/Volvo XC90 SUV 2007/00954.jpg')\nimgplot = plt.imshow(image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4ecb7f222aba762451502365474a6c7ad70b334"},"cell_type":"markdown","source":"Next step is read the class names from *names.csv*."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"class_names = []\n\nwith open(path_base + '/names.csv') as csvDataFile:\n    csvReader = csv.reader(csvDataFile, delimiter=';')\n    for row in csvReader:\n        class_names.append(row[0])\n\nprint(class_names)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3a4177804681c0af6ce5234a908b165c63cdeab"},"cell_type":"markdown","source":"## Building the model\nNow we will define the model architecture. First of all, load the Keras libraries."},{"metadata":{"trusted":true,"_uuid":"e5ff53f0f6ce10c4a6bf15c73fc5901e6a1f7ef1","collapsed":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import densenet\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\nfrom keras import regularizers\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62700908ca76e0a6f9b3a60ade318b2c98b09179"},"cell_type":"markdown","source":"And now, we set up the model parameters for training."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2964ba7294f89e22a6000dd498222dfafe456a45"},"cell_type":"code","source":"K.set_learning_phase(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c6939f9990b01c64cb952a47decd92b94ac15a3e"},"cell_type":"code","source":"img_width, img_height = 224, 224\nnb_train_samples = 8144\nnb_validation_samples = 8041\nepochs = 10\nbatch_size = 32\nn_classes = 196","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8fdf19bd1892793256f0cd489df419180352bd4"},"cell_type":"markdown","source":"In this step, we will use the Keras ImageDataGenerator methods for loading and transform the images of the dataset. With these methods, we don't need to load the entire image dataset in memory."},{"metadata":{"trusted":true,"_uuid":"bd5ea318a2f7878560d5a3dd620a01ee295a6ddd","collapsed":true},"cell_type":"code","source":"train_data_dir = path_base + '/car_data/train'\nvalidation_data_dir = path_base + '/car_data/test'\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    #shear_range=0.2,\n    zoom_range=0.2,\n    #fill_mode = 'constant',\n    #cval = 1,\n    rotation_range = 5,\n    #width_shift_range=0.2,\n    #height_shift_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bedc7a943d8ad9f0266d1e93ba8421224aa5933a"},"cell_type":"markdown","source":"We will use the default Keras DenseNet-121. The **imagenet** pretrained model will be the starting weights of our model. Then, we add the final layers for the model. These layers will use the **relu** activation function and the output layer will use **softmax**."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ab161bee9675e20453d488f7ee4b02ffe1f3faf9"},"cell_type":"code","source":"def build_model():\n    base_model = densenet.DenseNet121(input_shape=(img_width, img_height, 3),\n                                     weights='../input/full-keras-pretrained-no-top/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                                     include_top=False,\n                                     pooling='avg')\n    for layer in base_model.layers:\n      layer.trainable = True\n\n    x = base_model.output\n    x = Dense(1000, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)\n    x = Activation('relu')(x)\n    x = Dense(500, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)\n    x = Activation('relu')(x)\n    predictions = Dense(n_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"571850a23220347c8249e31d197df3f29d2343b5"},"cell_type":"markdown","source":"Now we create the model and compile setting the loss and optimization function and the metrics."},{"metadata":{"trusted":true,"_uuid":"933dce26b141f5c7d94673c486103d3057b65e1e","collapsed":true},"cell_type":"code","source":"model = build_model()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mse'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1d1611c6a7b37f7eb9ac5362f0544b61cd2645f"},"cell_type":"markdown","source":"The callbacks methods will be the *early stopping* if the model can't improve the loss for the validation dataset and *reduce learning rate* if this loss can't improve."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b10723b8f2434ff10dc7bef0fc4fd8f335c74f71"},"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss', patience=8, verbose=1, min_delta=1e-4)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_delta=1e-4)\ncallbacks_list = [early_stop, reduce_lr]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"269061b0543d0aabd8253c1d856eb7d47a4724e1"},"cell_type":"markdown","source":"## Training the model\nThe next step is to train the model. We specify the dataset to use for train and validate. We specify the number of epochs and the callbacks methods."},{"metadata":{"trusted":true,"_uuid":"c70bcdd124f7e72deb290a7b5eec9de2e092aa5a","collapsed":true},"cell_type":"code","source":"model_history = model.fit_generator(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=nb_validation_samples // batch_size,\n    callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"720671711ad32872638e3a72ce3c3bd61d6b2eac"},"cell_type":"markdown","source":"Now we have trained our model, we can see the metrics during the training proccess"},{"metadata":{"trusted":true,"_uuid":"b7a9efe002c0ce39f7668fd9814f09241088d835","collapsed":true},"cell_type":"code","source":"plt.figure(0)\nplt.plot(model_history.history['acc'],'r')\nplt.plot(model_history.history['val_acc'],'g')\nplt.xticks(np.arange(0, 20, 1.0))\nplt.rcParams['figure.figsize'] = (8, 6)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training Accuracy vs Validation Accuracy\")\nplt.legend(['train','validation'])\n \nplt.figure(1)\nplt.plot(model_history.history['loss'],'r')\nplt.plot(model_history.history['val_loss'],'g')\nplt.xticks(np.arange(0, 20, 1.0))\nplt.rcParams['figure.figsize'] = (8, 6)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss vs Validation Loss\")\nplt.legend(['train','validation'])\n\nplt.figure(2)\nplt.plot(model_history.history['mean_squared_error'],'r')\nplt.plot(model_history.history['val_mean_squared_error'],'g')\nplt.xticks(np.arange(0, 20, 1.0))\nplt.rcParams['figure.figsize'] = (8, 6)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"MSE\")\nplt.title(\"Training Loss vs Validation Loss\")\nplt.legend(['train','validation'])\n \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfdc6a54c3efdb81493f04173e8c827d09d8685e"},"cell_type":"markdown","source":"Now we evaluate the trained model with the validation dataset and make a prediction. The class predicted will be the class with maximum value for each image."},{"metadata":{"trusted":true,"_uuid":"32688054a80f06ac18b22b2cbfa5d9dec7a1605f","collapsed":true},"cell_type":"code","source":"model.evaluate_generator(validation_generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b57b42a213f0fb22a905f378b0aa9e6a209884b3","collapsed":true},"cell_type":"code","source":"pred = model.predict_generator(validation_generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)\npredicted = np.argmax(pred, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcf9a2448877aeb80c7bc2c1985bf673c3fb9b29"},"cell_type":"markdown","source":"## Confussion Matrix\nThe next step is to view the confusion matrix for the prediction results and the metris Precission, Recall and the F1-score that SciKit Learns library provides."},{"metadata":{"trusted":true,"_uuid":"9ca6966ad56aa181d8c89bf68a4f463f50f62d63","collapsed":true},"cell_type":"code","source":"print('Confusion Matrix')\ncm = confusion_matrix(validation_generator.classes, np.argmax(pred, axis=1))\nplt.figure(figsize = (30,20))\nsn.set(font_scale=1.4) #for label size\nsn.heatmap(cm, annot=True, annot_kws={\"size\": 12}) # font size\nplt.show()\nprint()\nprint('Classification Report')\nprint(classification_report(validation_generator.classes, predicted, target_names=class_names))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f1fa36f0ad98ea0da26279c27da5d7c66fc3795"},"cell_type":"markdown","source":"Finally, we can see some images with their class and their predictions."},{"metadata":{"trusted":true,"_uuid":"ef3e038af1c930f9df64b4063afd72777c675623","collapsed":true},"cell_type":"code","source":"def predict_one(model):\n    image_batch, classes_batch = next(validation_generator)\n    predicted_batch = model.predict(image_batch)\n    for k in range(0,image_batch.shape[0]):\n      image = image_batch[k]\n      pred = predicted_batch[k]\n      the_pred = np.argmax(pred)\n      predicted = class_names[the_pred]\n      val_pred = max(pred)\n      the_class = np.argmax(classes_batch[k])\n      value = class_names[np.argmax(classes_batch[k])]\n      plt.figure(k)\n      isTrue = (the_pred == the_class)\n      plt.title(str(isTrue) + ' - class: ' + value + ' - ' + 'predicted: ' + predicted + '[' + str(val_pred) + ']')\n      plt.imshow(image)\n\npredict_one(model)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}